2026-02-05 14:41:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'NodeScrapy.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2026-02-05 14:41:06 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py:44: ScrapyDeprecationWarning: RandomUserAgentMiddleware.process_request() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(mw.process_request)

2026-02-05 14:41:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2026-02-05 14:41:06 [scrapy.middleware] INFO: Enabled item pipelines:
['NodeScrapy.pipelines.Pipeline']
2026-02-05 14:41:06 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/pipelines/__init__.py:41: ScrapyDeprecationWarning: Pipeline.open_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.open_spider)

2026-02-05 14:41:06 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/pipelines/__init__.py:44: ScrapyDeprecationWarning: Pipeline.close_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.close_spider)

2026-02-05 14:41:06 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/pipelines/__init__.py:47: ScrapyDeprecationWarning: Pipeline.process_item() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.process_item)

2026-02-05 14:41:06 [scrapy.core.engine] INFO: Spider opened
2026-02-05 14:41:06 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py:490: ScrapyDeprecationWarning: NodeScrapy.spiders.SimpleSpider.SimpleSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2026-02-05 14:41:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2026-02-05 14:41:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2026-02-05 14:41:06 [simple] INFO: clashmeta start
2026-02-05 14:41:06 [simple] INFO: ndnode start
2026-02-05 14:41:06 [simple] INFO: nodev2ray start
2026-02-05 14:41:06 [simple] INFO: nodefree start
2026-02-05 14:41:06 [simple] INFO: v2rayshare start
2026-02-05 14:41:06 [simple] INFO: wenode start
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-2-5-free-node-subscribe-links.htm on 2026-02-05
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-2-5-free-node-subscribe-links.htm on 2026-02-05
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-2-4-latest-clash-meta-node.htm on 2026-02-04
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-2-4-latest-clash-meta-node.htm on 2026-02-04
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-2-3-clash-meta-node-github.htm on 2026-02-03
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-2-3-clash-meta-node-github.htm on 2026-02-03
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-2-2-free-clash-meta-node.htm on 2026-02-02
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-2-2-free-clash-meta-node.htm on 2026-02-02
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-2-1-free-node-subscribe.htm on 2026-02-01
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-2-1-free-node-subscribe.htm on 2026-02-01
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-1-31-free-high-speed-nodes.htm on 2026-01-31
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-1-31-free-high-speed-nodes.htm on 2026-01-31
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-1-30-node-share.htm on 2026-01-30
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-1-30-node-share.htm on 2026-01-30
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-1-29-clash-meta-node-share.htm on 2026-01-29
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-1-29-clash-meta-node-share.htm on 2026-01-29
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-1-28-free-node-subscribe.htm on 2026-01-28
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-1-28-free-node-subscribe.htm on 2026-01-28
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-1-27-free-clash-meta-node.htm on 2026-01-27
2026-02-05 14:41:06 [simple] INFO: clashmeta found /free-nodes/2026-1-27-free-clash-meta-node.htm on 2026-01-27
2026-02-05 14:41:06 [simple] INFO: clashmeta is up to date, exiting
2026-02-05 14:41:06 [simple] INFO: clashmeta is up to date, exiting
2026-02-05 14:41:06 [simple] INFO: clashmeta is up to date, exiting
2026-02-05 14:41:06 [simple] INFO: nodev2ray found /free-node/2026-2-4-node-share.htm on 2026-02-04
2026-02-05 14:41:06 [simple] INFO: nodev2ray found /free-node/2026-2-3-free-ssr-node.htm on 2026-02-03
2026-02-05 14:41:06 [simple] INFO: nodev2ray found /free-node/2026-2-2-shadowrocket-node.htm on 2026-02-02
2026-02-05 14:41:06 [simple] INFO: nodev2ray found /free-node/2026-2-1-free-shadowrocket-node.htm on 2026-02-01
2026-02-05 14:41:06 [simple] INFO: nodev2ray found /free-node/2026-1-31-free-clash-subscribe.htm on 2026-01-31
2026-02-05 14:41:06 [simple] INFO: nodev2ray found /free-node/2026-1-30-clash-v2ray-ss-ssr.htm on 2026-01-30
2026-02-05 14:41:06 [simple] INFO: nodev2ray found /free-node/2026-1-29-free-v2ray-subscribe.htm on 2026-01-29
2026-02-05 14:41:06 [simple] INFO: nodev2ray found /free-node/2026-1-28-shadowrocket-node.htm on 2026-01-28
2026-02-05 14:41:06 [simple] INFO: nodev2ray found /free-node/2026-1-27-v2ray-windows.htm on 2026-01-27
2026-02-05 14:41:06 [simple] INFO: nodev2ray found /free-node/2026-1-26-free-v2ray.htm on 2026-01-26
2026-02-05 14:41:06 [simple] INFO: nodev2ray is up to date, exiting
2026-02-05 14:41:06 [simple] INFO: nodev2ray is up to date, exiting
2026-02-05 14:41:06 [simple] INFO: nodev2ray is up to date, exiting
2026-02-05 14:41:06 [simple] INFO: ndnode found https://www.naidounode.com/n/3543.html on 2026-02-05
2026-02-05 14:41:06 [simple] INFO: ndnode found https://www.naidounode.com/n/3541.html on 2026-02-04
2026-02-05 14:41:06 [simple] INFO: ndnode found https://www.naidounode.com/n/3539.html on 2026-02-03
2026-02-05 14:41:06 [simple] INFO: ndnode found https://www.naidounode.com/n/3537.html on 2026-02-02
2026-02-05 14:41:06 [simple] INFO: ndnode found https://www.naidounode.com/n/3535.html on 2026-02-01
2026-02-05 14:41:06 [simple] INFO: ndnode found https://www.naidounode.com/n/3533.html on 2026-01-31
2026-02-05 14:41:06 [simple] INFO: ndnode found https://www.naidounode.com/n/3531.html on 2026-01-30
2026-02-05 14:41:06 [simple] INFO: ndnode found https://www.naidounode.com/n/3529.html on 2026-01-29
2026-02-05 14:41:06 [simple] INFO: ndnode found https://www.naidounode.com/n/3527.html on 2026-01-28
2026-02-05 14:41:06 [simple] INFO: ndnode found https://www.naidounode.com/n/3525.html on 2026-01-27
2026-02-05 14:41:06 [simple] INFO: ndnode is up to date, exiting
2026-02-05 14:41:06 [simple] INFO: ndnode is up to date, exiting
2026-02-05 14:41:06 [simple] INFO: ndnode is up to date, exiting
2026-02-05 14:41:07 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://v2rayshare.githubrowcontent.com/2024/07/20240701.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: v2rayshare.githubrowcontent.com.
2026-02-05 14:41:07 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://v2rayshare.net/>: DNS lookup failed: no results for hostname lookup: v2rayshare.githubrowcontent.com.
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/downloadermiddlewares/robotstxt.py", line 100, in robot_parser
    resp = await self.crawler.engine.download_async(robotsreq)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 474, in download_async
    return await self.download_async(response_or_request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 474, in download_async
    return await self.download_async(response_or_request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 474, in download_async
    return await self.download_async(response_or_request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 467, in download_async
    response_or_request = await maybe_deferred_to_future(
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/python/failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 490, in _download
    result = yield self.downloader.fetch(request)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/python/failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 144, in fetch
    yield deferred_from_coro(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1257, in adapt
    extracted: _SelfResultT | Failure = result.result()
                                        ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 158, in download_async
    result = await process_exception(ex)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 150, in process_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 153, in download_async
    result: Response | Request = await process_request(request)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 97, in process_request
    return await download_func(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 202, in _enqueue_request
    return await maybe_deferred_to_future(d)  # fired in _wait_for_download()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 269, in _wait_for_download
    response = await self._download(slot, request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 239, in _download
    response: Response = await self.handlers.download_request_async(request)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/handlers/__init__.py", line 156, in download_request_async
    return await handler.download_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/handlers/http11.py", line 113, in download_request
    return await maybe_deferred_to_future(agent.download_request(request))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1092, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/endpoints.py", line 1091, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: v2rayshare.githubrowcontent.com.
2026-02-05 14:41:07 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://nodefree.githubrowcontent.com/2024/07/20240705.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: nodefree.githubrowcontent.com.
2026-02-05 14:41:07 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://nodefree.org>: DNS lookup failed: no results for hostname lookup: nodefree.githubrowcontent.com.
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/downloadermiddlewares/robotstxt.py", line 100, in robot_parser
    resp = await self.crawler.engine.download_async(robotsreq)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 474, in download_async
    return await self.download_async(response_or_request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 474, in download_async
    return await self.download_async(response_or_request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 474, in download_async
    return await self.download_async(response_or_request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 467, in download_async
    response_or_request = await maybe_deferred_to_future(
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/python/failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 490, in _download
    result = yield self.downloader.fetch(request)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/python/failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 144, in fetch
    yield deferred_from_coro(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1257, in adapt
    extracted: _SelfResultT | Failure = result.result()
                                        ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 158, in download_async
    result = await process_exception(ex)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 150, in process_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 153, in download_async
    result: Response | Request = await process_request(request)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 97, in process_request
    return await download_func(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 202, in _enqueue_request
    return await maybe_deferred_to_future(d)  # fired in _wait_for_download()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 269, in _wait_for_download
    response = await self._download(slot, request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 239, in _download
    response: Response = await self.handlers.download_request_async(request)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/handlers/__init__.py", line 156, in download_request_async
    return await handler.download_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/handlers/http11.py", line 113, in download_request
    return await maybe_deferred_to_future(agent.download_request(request))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1092, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/endpoints.py", line 1091, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: nodefree.githubrowcontent.com.
2026-02-05 14:41:07 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5287.html on 2026-02-05
2026-02-05 14:41:07 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5288.html on 2026-02-04
2026-02-05 14:41:07 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5289.html on 2026-02-03
2026-02-05 14:41:07 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5290.html on 2026-02-02
2026-02-05 14:41:07 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5291.html on 2026-02-01
2026-02-05 14:41:07 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5292.html on 2026-01-31
2026-02-05 14:41:07 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5293.html on 2026-01-30
2026-02-05 14:41:07 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5294.html on 2026-01-29
2026-02-05 14:41:07 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5295.html on 2026-01-28
2026-02-05 14:41:07 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5296.html on 2026-01-27
2026-02-05 14:41:07 [simple] INFO: v2rayshare needs update, accessing https://v2rayshare.net/p/5287.html
2026-02-05 14:41:07 [simple] INFO: v2rayshare needs update, accessing https://v2rayshare.net/p/5288.html
2026-02-05 14:41:07 [simple] INFO: v2rayshare needs update, accessing https://v2rayshare.net/p/5289.html
2026-02-05 14:41:07 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://oneclash.githubrowcontent.com/2024/06/20240601.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: oneclash.githubrowcontent.com.
2026-02-05 14:41:07 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://oneclash.cc>: DNS lookup failed: no results for hostname lookup: oneclash.githubrowcontent.com.
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/downloadermiddlewares/robotstxt.py", line 100, in robot_parser
    resp = await self.crawler.engine.download_async(robotsreq)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 474, in download_async
    return await self.download_async(response_or_request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 474, in download_async
    return await self.download_async(response_or_request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 474, in download_async
    return await self.download_async(response_or_request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 467, in download_async
    response_or_request = await maybe_deferred_to_future(
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/python/failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 490, in _download
    result = yield self.downloader.fetch(request)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/python/failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 144, in fetch
    yield deferred_from_coro(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1257, in adapt
    extracted: _SelfResultT | Failure = result.result()
                                        ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 158, in download_async
    result = await process_exception(ex)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 150, in process_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 153, in download_async
    result: Response | Request = await process_request(request)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 97, in process_request
    return await download_func(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 202, in _enqueue_request
    return await maybe_deferred_to_future(d)  # fired in _wait_for_download()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 269, in _wait_for_download
    response = await self._download(slot, request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 239, in _download
    response: Response = await self.handlers.download_request_async(request)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/handlers/__init__.py", line 156, in download_request_async
    return await handler.download_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/handlers/http11.py", line 113, in download_request
    return await maybe_deferred_to_future(agent.download_request(request))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1092, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/endpoints.py", line 1091, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: oneclash.githubrowcontent.com.
2026-02-05 14:41:07 [simple] INFO: wenode found https://oneclash.cc/a/3516.html on 2026-02-05
2026-02-05 14:41:07 [simple] INFO: wenode found https://oneclash.cc/a/3517.html on 2026-02-04
2026-02-05 14:41:07 [simple] INFO: wenode found https://oneclash.cc/a/3518.html on 2026-02-03
2026-02-05 14:41:07 [simple] INFO: wenode found https://oneclash.cc/a/3519.html on 2026-02-02
2026-02-05 14:41:07 [simple] INFO: wenode found https://oneclash.cc/a/3520.html on 2026-02-01
2026-02-05 14:41:07 [simple] INFO: wenode found https://oneclash.cc/a/3521.html on 2026-01-31
2026-02-05 14:41:07 [simple] INFO: wenode found https://oneclash.cc/a/3522.html on 2026-01-30
2026-02-05 14:41:07 [simple] INFO: wenode found https://oneclash.cc/a/3523.html on 2026-01-29
2026-02-05 14:41:07 [simple] INFO: wenode found https://oneclash.cc/a/3524.html on 2026-01-28
2026-02-05 14:41:07 [simple] INFO: wenode found https://oneclash.cc/a/3525.html on 2026-01-27
2026-02-05 14:41:07 [simple] INFO: wenode needs update, accessing https://oneclash.cc/a/3516.html
2026-02-05 14:41:07 [simple] INFO: wenode needs update, accessing https://oneclash.cc/a/3517.html
2026-02-05 14:41:07 [simple] INFO: wenode needs update, accessing https://oneclash.cc/a/3518.html
2026-02-05 14:41:08 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://nodefree.githubrowcontent.com/2024/07/20240705.txt> (failed 3 times): DNS lookup failed: no results for hostname lookup: nodefree.githubrowcontent.com.
2026-02-05 14:41:08 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://nodefree.me/>: DNS lookup failed: no results for hostname lookup: nodefree.githubrowcontent.com.
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/downloadermiddlewares/robotstxt.py", line 100, in robot_parser
    resp = await self.crawler.engine.download_async(robotsreq)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 474, in download_async
    return await self.download_async(response_or_request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 474, in download_async
    return await self.download_async(response_or_request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 474, in download_async
    return await self.download_async(response_or_request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 467, in download_async
    response_or_request = await maybe_deferred_to_future(
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/python/failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 490, in _download
    result = yield self.downloader.fetch(request)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/python/failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 144, in fetch
    yield deferred_from_coro(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1257, in adapt
    extracted: _SelfResultT | Failure = result.result()
                                        ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 158, in download_async
    result = await process_exception(ex)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 150, in process_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 153, in download_async
    result: Response | Request = await process_request(request)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 97, in process_request
    return await download_func(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 202, in _enqueue_request
    return await maybe_deferred_to_future(d)  # fired in _wait_for_download()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 269, in _wait_for_download
    response = await self._download(slot, request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 239, in _download
    response: Response = await self.handlers.download_request_async(request)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/handlers/__init__.py", line 156, in download_request_async
    return await handler.download_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/handlers/http11.py", line 113, in download_request
    return await maybe_deferred_to_future(agent.download_request(request))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1092, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/endpoints.py", line 1091, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: nodefree.githubrowcontent.com.
2026-02-05 14:41:08 [simple] INFO: nodefree found https://nodefree.me/p/3884.html on 2026-02-05
2026-02-05 14:41:08 [simple] INFO: nodefree found https://nodefree.me/p/3885.html on 2026-02-04
2026-02-05 14:41:08 [simple] INFO: nodefree found https://nodefree.me/p/3886.html on 2026-02-03
2026-02-05 14:41:08 [simple] INFO: nodefree found https://nodefree.me/p/3887.html on 2026-02-02
2026-02-05 14:41:08 [simple] INFO: nodefree found https://nodefree.me/p/3888.html on 2026-02-01
2026-02-05 14:41:08 [simple] INFO: nodefree found https://nodefree.me/p/3889.html on 2026-01-31
2026-02-05 14:41:08 [simple] INFO: nodefree found https://nodefree.me/p/3890.html on 2026-01-30
2026-02-05 14:41:08 [simple] INFO: nodefree found https://nodefree.me/p/3891.html on 2026-01-29
2026-02-05 14:41:08 [simple] INFO: nodefree found https://nodefree.me/p/3892.html on 2026-01-28
2026-02-05 14:41:08 [simple] INFO: nodefree found https://nodefree.me/p/3893.html on 2026-01-27
2026-02-05 14:41:08 [simple] INFO: nodefree needs update, accessing https://nodefree.me/p/3884.html
2026-02-05 14:41:08 [simple] INFO: nodefree needs update, accessing https://nodefree.me/p/3885.html
2026-02-05 14:41:08 [simple] INFO: nodefree needs update, accessing https://nodefree.me/p/3886.html
2026-02-05 14:41:08 [scrapy.core.engine] INFO: Closing spider (finished)
2026-02-05 14:41:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 12,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 12,
 'downloader/request_bytes': 12981,
 'downloader/request_count': 39,
 'downloader/request_method_count/GET': 39,
 'downloader/response_bytes': 167565,
 'downloader/response_count': 27,
 'downloader/response_status_count/200': 19,
 'downloader/response_status_count/301': 8,
 'elapsed_time_seconds': 2.661212,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2026, 2, 5, 14, 41, 8, 775861, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 722547,
 'httpcompression/response_count': 18,
 'items_per_minute': 0.0,
 'log_count/ERROR': 8,
 'log_count/INFO': 97,
 'memusage/max': 154091520,
 'memusage/startup': 154091520,
 'request_depth_max': 1,
 'response_received_count': 19,
 'responses_per_minute': 570.0,
 'retry/count': 8,
 'retry/max_reached': 4,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 8,
 "robotstxt/exception_count/<class 'twisted.internet.error.DNSLookupError'>": 4,
 'robotstxt/request_count': 8,
 'robotstxt/response_count': 4,
 'robotstxt/response_status_count/200': 4,
 'scheduler/dequeued': 18,
 'scheduler/dequeued/memory': 18,
 'scheduler/enqueued': 18,
 'scheduler/enqueued/memory': 18,
 'start_time': datetime.datetime(2026, 2, 5, 14, 41, 6, 114649, tzinfo=datetime.timezone.utc)}
2026-02-05 14:41:08 [scrapy.core.engine] INFO: Spider closed (finished)
2026-02-05 14:41:11 [scrapy.addons] INFO: Enabled addons:
[]
2026-02-05 14:41:11 [scrapy.extensions.telnet] INFO: Telnet Password: 3230cefca0eae73e
2026-02-05 14:41:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logcount.LogCount',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2026-02-05 14:41:11 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'NodeScrapy',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'NodeScrapy.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['NodeScrapy.spiders']}
2026-02-05 14:41:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'NodeScrapy.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2026-02-05 14:41:11 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py:44: ScrapyDeprecationWarning: RandomUserAgentMiddleware.process_request() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(mw.process_request)

2026-02-05 14:41:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2026-02-05 14:41:11 [scrapy.middleware] INFO: Enabled item pipelines:
['NodeScrapy.pipelines.Pipeline']
2026-02-05 14:41:11 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/pipelines/__init__.py:41: ScrapyDeprecationWarning: Pipeline.open_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.open_spider)

2026-02-05 14:41:11 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/pipelines/__init__.py:44: ScrapyDeprecationWarning: Pipeline.close_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.close_spider)

2026-02-05 14:41:11 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/pipelines/__init__.py:47: ScrapyDeprecationWarning: Pipeline.process_item() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.process_item)

2026-02-05 14:41:11 [scrapy.core.engine] INFO: Spider opened
2026-02-05 14:41:11 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py:490: ScrapyDeprecationWarning: NodeScrapy.spiders.SimpleSpider.SimpleSpider (inherited by NodeScrapy.spiders.DecryptSpider.DecryptSpider) defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2026-02-05 14:41:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2026-02-05 14:41:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2026-02-05 14:41:11 [decrypt] INFO: yudou66 start
2026-02-05 14:41:11 [decrypt] INFO: blues start
2026-02-05 14:41:12 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/02/2026234kchatgpt4k8kclashv2ray.html on 2026-02-03
2026-02-05 14:41:12 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/2026224kchatgpt4k8kclashv2ray.html on 2026-02-02
2026-02-05 14:41:12 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261314kchatgpt4k8kclashv2ray.html on 2026-02-01
2026-02-05 14:41:12 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261314kchatgpt4k8kclashv2ray_30.html on 2026-01-31
2026-02-05 14:41:12 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261304kchatgpt4k8kclashv2ray.html on 2026-01-30
2026-02-05 14:41:12 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261284kchatgpt4k8kclashv2ray.html on 2026-01-28
2026-02-05 14:41:12 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261294kchatgpt4k8kclashv2ray.html on 2026-01-29
2026-02-05 14:41:12 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261274kchatgpt4k8kclashv2ray.html on 2026-01-27
2026-02-05 14:41:12 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261254kchatgpt4k8kclashv2ray.html on 2026-01-25
2026-02-05 14:41:12 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261244kchatgpt4k8kclashv2ray.html on 2026-01-24
2026-02-05 14:41:12 [decrypt] INFO: blues needs update, accessing https://blues2022.blogspot.com/2026/02/2026234kchatgpt4k8kclashv2ray.html
2026-02-05 14:41:12 [decrypt] INFO: blues needs update, accessing https://blues2022.blogspot.com/2026/01/2026224kchatgpt4k8kclashv2ray.html
2026-02-05 14:41:12 [decrypt] INFO: blues needs update, accessing https://blues2022.blogspot.com/2026/01/20261314kchatgpt4k8kclashv2ray.html
2026-02-05 14:41:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.yudou123.top/robots.txt> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2026-02-05 14:41:12 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://www.yudou123.top/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/downloadermiddlewares/robotstxt.py", line 100, in robot_parser
    resp = await self.crawler.engine.download_async(robotsreq)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 474, in download_async
    return await self.download_async(response_or_request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 474, in download_async
    return await self.download_async(response_or_request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 467, in download_async
    response_or_request = await maybe_deferred_to_future(
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/python/failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 490, in _download
    result = yield self.downloader.fetch(request)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/python/failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 144, in fetch
    yield deferred_from_coro(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1257, in adapt
    extracted: _SelfResultT | Failure = result.result()
                                        ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 158, in download_async
    result = await process_exception(ex)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 150, in process_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 153, in download_async
    result: Response | Request = await process_request(request)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 97, in process_request
    return await download_func(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 202, in _enqueue_request
    return await maybe_deferred_to_future(d)  # fired in _wait_for_download()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 269, in _wait_for_download
    response = await self._download(slot, request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 239, in _download
    response: Response = await self.handlers.download_request_async(request)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/handlers/__init__.py", line 156, in download_request_async
    return await handler.download_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/handlers/http11.py", line 113, in download_request
    return await maybe_deferred_to_future(agent.download_request(request))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2026-02-05 14:41:12 [decrypt] INFO: blues found yt_url: https://youtu.be/wfMFZi7XWbQ
2026-02-05 14:41:12 [pytubefix.__main__] WARNING: `use_po_token` and `po_token_verifier` is deprecated and will be removed soon.
2026-02-05 14:41:13 [decrypt] ERROR: blues found yt_url error: EOF when reading a line
2026-02-05 14:41:15 [decrypt] WARNING: blues 1155 got 
2026-02-05 14:41:17 [decrypt] WARNING: blues 0000 got 
2026-02-05 14:41:18 [decrypt] WARNING: blues 0011 got 
2026-02-05 14:41:19 [decrypt] WARNING: blues 0022 got 
2026-02-05 14:41:21 [decrypt] WARNING: blues 0033 got 
2026-02-05 14:41:23 [decrypt] WARNING: blues 0044 got 
2026-02-05 14:41:24 [decrypt] WARNING: blues 0055 got 
2026-02-05 14:41:26 [decrypt] WARNING: blues 0066 got 
2026-02-05 14:41:27 [decrypt] WARNING: blues 0077 got 
2026-02-05 14:41:28 [decrypt] WARNING: blues 0088 got 
2026-02-05 14:41:30 [decrypt] WARNING: blues 0099 got 
2026-02-05 14:41:32 [decrypt] WARNING: blues 1100 got 
2026-02-05 14:41:33 [decrypt] WARNING: blues 1111 got 
2026-02-05 14:41:34 [decrypt] WARNING: blues 1122 got 
2026-02-05 14:41:36 [decrypt] WARNING: blues 1133 got 
2026-02-05 14:41:38 [decrypt] WARNING: blues 1144 got 
2026-02-05 14:41:39 [decrypt] WARNING: blues 1155 got 
2026-02-05 14:41:40 [decrypt] WARNING: blues 1166 got 
2026-02-05 14:41:42 [decrypt] WARNING: blues 1177 got 
2026-02-05 14:41:43 [decrypt] WARNING: blues 1188 got 
2026-02-05 14:41:44 [decrypt] WARNING: blues 1199 got 
2026-02-05 14:41:46 [decrypt] WARNING: blues 2200 got 
2026-02-05 14:41:47 [decrypt] WARNING: blues 2211 got 
2026-02-05 14:41:48 [decrypt] WARNING: blues 2222 got 
2026-02-05 14:41:50 [decrypt] WARNING: blues 2233 got 
2026-02-05 14:41:51 [decrypt] WARNING: blues 2244 got 
2026-02-05 14:41:52 [decrypt] WARNING: blues 2255 got 
2026-02-05 14:41:54 [decrypt] WARNING: blues 2266 got 
2026-02-05 14:41:55 [decrypt] WARNING: blues 2277 got 
2026-02-05 14:41:56 [decrypt] WARNING: blues 2288 got 
2026-02-05 14:41:58 [decrypt] WARNING: blues 2299 got 
2026-02-05 14:41:59 [decrypt] WARNING: blues 3300 got 
2026-02-05 14:42:01 [decrypt] WARNING: blues 3311 got 
2026-02-05 14:42:02 [decrypt] WARNING: blues 3322 got 
2026-02-05 14:42:04 [decrypt] WARNING: blues 3333 got 
2026-02-05 14:42:05 [decrypt] WARNING: blues 3344 got 
2026-02-05 14:42:06 [decrypt] WARNING: blues 3355 got 
2026-02-05 14:42:07 [decrypt] WARNING: blues 3366 got 
2026-02-05 14:42:09 [decrypt] WARNING: blues 3377 got 
2026-02-05 14:42:10 [decrypt] WARNING: blues 3388 got 
2026-02-05 14:42:11 [decrypt] WARNING: blues 3399 got 
2026-02-05 14:42:13 [decrypt] WARNING: blues 4400 got 
2026-02-05 14:42:14 [decrypt] WARNING: blues 4411 got 
2026-02-05 14:42:15 [decrypt] WARNING: blues 4422 got 
2026-02-05 14:42:17 [decrypt] WARNING: blues 4433 got 
2026-02-05 14:42:18 [decrypt] WARNING: blues 4444 got 
2026-02-05 14:42:19 [decrypt] WARNING: blues 4455 got 
2026-02-05 14:42:21 [decrypt] WARNING: blues 4466 got 
2026-02-05 14:42:22 [decrypt] WARNING: blues 4477 got 
2026-02-05 14:42:23 [decrypt] WARNING: blues 4488 got 
2026-02-05 14:42:24 [decrypt] WARNING: blues 4499 got 
2026-02-05 14:42:26 [decrypt] WARNING: blues 5500 got 
2026-02-05 14:42:27 [decrypt] WARNING: blues 5511 got 
2026-02-05 14:42:28 [decrypt] WARNING: blues 5522 got 
2026-02-05 14:42:30 [decrypt] WARNING: blues 5533 got 
2026-02-05 14:42:31 [decrypt] WARNING: blues 5544 got 
2026-02-05 14:42:32 [decrypt] WARNING: blues 5555 got 
2026-02-05 14:42:33 [decrypt] WARNING: blues 5566 got 
2026-02-05 14:42:35 [decrypt] WARNING: blues 5577 got 
2026-02-05 14:42:36 [decrypt] WARNING: blues 5588 got 
2026-02-05 14:42:38 [decrypt] WARNING: blues 5599 got 
2026-02-05 14:42:39 [decrypt] WARNING: blues 6600 got 
2026-02-05 14:42:40 [decrypt] WARNING: blues 6611 got 
2026-02-05 14:42:41 [decrypt] WARNING: blues 6622 got 
2026-02-05 14:42:43 [decrypt] WARNING: blues 6633 got 
2026-02-05 14:42:44 [decrypt] WARNING: blues 6644 got 
2026-02-05 14:42:45 [decrypt] WARNING: blues 6655 got 
2026-02-05 14:42:46 [decrypt] WARNING: blues 6666 got 
2026-02-05 14:42:48 [decrypt] WARNING: blues 6677 got 
2026-02-05 14:42:49 [decrypt] WARNING: blues 6688 got 
2026-02-05 14:42:50 [decrypt] WARNING: blues 6699 got 
2026-02-05 14:42:52 [decrypt] WARNING: blues 7700 got 
2026-02-05 14:42:53 [decrypt] WARNING: blues 7711 got 
2026-02-05 14:42:54 [decrypt] WARNING: blues 7722 got 
2026-02-05 14:42:55 [decrypt] WARNING: blues 7733 got 
2026-02-05 14:42:56 [decrypt] WARNING: blues 7744 got 
2026-02-05 14:42:58 [decrypt] WARNING: blues 7755 got 
2026-02-05 14:43:00 [decrypt] WARNING: blues 7766 got 
2026-02-05 14:43:01 [decrypt] WARNING: blues 7777 got 
2026-02-05 14:43:02 [decrypt] WARNING: blues 7788 got 
2026-02-05 14:43:03 [decrypt] WARNING: blues 7799 got 
2026-02-05 14:43:05 [decrypt] WARNING: blues 8800 got 
2026-02-05 14:43:06 [decrypt] WARNING: blues 8811 got 
2026-02-05 14:43:07 [decrypt] WARNING: blues 8822 got 
2026-02-05 14:43:09 [decrypt] WARNING: blues 8833 got 
2026-02-05 14:43:10 [decrypt] WARNING: blues 8844 got 
2026-02-05 14:43:12 [decrypt] WARNING: blues 8855 got 
2026-02-05 14:43:13 [decrypt] WARNING: blues 8866 got 
2026-02-05 14:43:15 [decrypt] WARNING: blues 8877 got 
2026-02-05 14:43:16 [decrypt] WARNING: blues 8888 got 
2026-02-05 14:43:17 [decrypt] WARNING: blues 8899 got 
2026-02-05 14:43:18 [decrypt] WARNING: blues 9900 got 
2026-02-05 14:43:20 [decrypt] WARNING: blues 9911 got 
2026-02-05 14:43:23 [decrypt] INFO: blues saved new password 9922
2026-02-05 14:43:23 [decrypt] INFO: blues found yt_url: https://youtu.be/wfMFZi7XWbQ
2026-02-05 14:43:23 [pytubefix.__main__] WARNING: `use_po_token` and `po_token_verifier` is deprecated and will be removed soon.
2026-02-05 14:43:23 [decrypt] ERROR: blues found yt_url error: EOF when reading a line
2026-02-05 14:43:24 [decrypt] WARNING: blues 9922 got 
2026-02-05 14:43:26 [decrypt] WARNING: blues 0000 got 
2026-02-05 14:43:27 [decrypt] WARNING: blues 0011 got 
2026-02-05 14:43:28 [decrypt] WARNING: blues 0022 got 
2026-02-05 14:43:30 [decrypt] WARNING: blues 0033 got 
2026-02-05 14:43:31 [decrypt] WARNING: blues 0044 got 
2026-02-05 14:43:32 [decrypt] WARNING: blues 0055 got 
2026-02-05 14:43:33 [decrypt] WARNING: blues 0066 got 
2026-02-05 14:43:34 [decrypt] WARNING: blues 0077 got 
2026-02-05 14:43:36 [decrypt] WARNING: blues 0088 got 
2026-02-05 14:43:36 [decrypt] WARNING: blues 0099 got 
2026-02-05 14:43:37 [decrypt] WARNING: blues 1100 got 
2026-02-05 14:43:39 [decrypt] WARNING: blues 1111 got 
2026-02-05 14:43:40 [decrypt] WARNING: blues 1122 got 
2026-02-05 14:43:41 [decrypt] WARNING: blues 1133 got 
2026-02-05 14:43:42 [decrypt] WARNING: blues 1144 got 
2026-02-05 14:43:43 [decrypt] WARNING: blues 1155 got 
2026-02-05 14:43:45 [decrypt] WARNING: blues 1166 got 
2026-02-05 14:43:45 [decrypt] WARNING: blues 1177 got 
2026-02-05 14:43:46 [decrypt] WARNING: blues 1188 got 
2026-02-05 14:43:48 [decrypt] WARNING: blues 1199 got 
2026-02-05 14:43:49 [decrypt] WARNING: blues 2200 got 
2026-02-05 14:43:50 [decrypt] WARNING: blues 2211 got 
2026-02-05 14:43:51 [decrypt] WARNING: blues 2222 got 
2026-02-05 14:44:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://blues2022.blogspot.com/2026/01/2026224kchatgpt4k8kclashv2ray.html> (referer: https://blues2022.blogspot.com)
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/utils/defer.py", line 355, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/utils/python.py", line 311, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/utils/python.py", line 311, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 60, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 60, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 60, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 61, in process_spider_output
    yield from super().process_spider_output(response, result)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 60, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/home/runner/work/FreeNodes/FreeNodes/NodeScrapy/spiders/DecryptSpider.py", line 91, in parse_blog
    ok, msg = self._decrypt(response.url, method, pwd)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/runner/work/FreeNodes/FreeNodes/NodeScrapy/spiders/DecryptSpider.py", line 45, in _decrypt
    wait.until(EC.presence_of_all_elements_located((By.TAG_NAME, "button")))
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 121, in until
    raise TimeoutException(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: 

2026-02-05 14:44:02 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2026-02-05 14:44:02 [decrypt] INFO: blues found yt_url: https://youtu.be/wfMFZi7XWbQ
2026-02-05 14:44:02 [pytubefix.__main__] WARNING: `use_po_token` and `po_token_verifier` is deprecated and will be removed soon.
2026-02-05 14:44:03 [decrypt] ERROR: blues found yt_url error: EOF when reading a line
2026-02-05 14:44:07 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.yudou123.top/> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2026-02-05 14:44:07 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.yudou123.top/>
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/python/failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/engine.py", line 490, in _download
    result = yield self.downloader.fetch(request)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/python/failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 144, in fetch
    yield deferred_from_coro(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1257, in adapt
    extracted: _SelfResultT | Failure = result.result()
                                        ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 158, in download_async
    result = await process_exception(ex)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 150, in process_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 153, in download_async
    result: Response | Request = await process_request(request)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py", line 97, in process_request
    return await download_func(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 202, in _enqueue_request
    return await maybe_deferred_to_future(d)  # fired in _wait_for_download()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 269, in _wait_for_download
    response = await self._download(slot, request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/__init__.py", line 239, in _download
    response: Response = await self.handlers.download_request_async(request)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/handlers/__init__.py", line 156, in download_request_async
    return await handler.download_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/handlers/http11.py", line 113, in download_request
    return await maybe_deferred_to_future(agent.download_request(request))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2026-02-05 14:44:07 [scrapy.core.engine] INFO: Closing spider (finished)
2026-02-05 14:44:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 6,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 6,
 'downloader/request_bytes': 4326,
 'downloader/request_count': 13,
 'downloader/request_method_count/GET': 13,
 'downloader/response_bytes': 167484,
 'downloader/response_count': 7,
 'downloader/response_status_count/200': 5,
 'downloader/response_status_count/302': 2,
 'elapsed_time_seconds': 175.949366,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2026, 2, 5, 14, 44, 7, 449818, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 972108,
 'httpcompression/response_count': 5,
 'items_per_minute': 0.0,
 'log_count/ERROR': 8,
 'log_count/INFO': 23,
 'log_count/WARNING': 120,
 'memusage/max': 170295296,
 'memusage/startup': 153264128,
 'request_depth_max': 1,
 'response_received_count': 5,
 'responses_per_minute': 1.7142857142857144,
 'retry/count': 4,
 'retry/max_reached': 2,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 4,
 "robotstxt/exception_count/<class 'twisted.web._newclient.ResponseNeverReceived'>": 1,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 8,
 'scheduler/dequeued/memory': 8,
 'scheduler/enqueued': 8,
 'scheduler/enqueued/memory': 8,
 'spider_exceptions/TimeoutException': 1,
 'spider_exceptions/count': 1,
 'start_time': datetime.datetime(2026, 2, 5, 14, 41, 11, 500452, tzinfo=datetime.timezone.utc)}
2026-02-05 14:44:07 [scrapy.core.engine] INFO: Spider closed (finished)
